Title         : Data Mining Tutorial Summary
Author        : Tobias Knöppler

[TITLE]

#Measurements
*n = sample Size; N = population size*

- Mean (sample vs population)
  ~ Equation {#mean-default}
    x = \frac{1}{n}\sum_{i=1}^n x_i
  ~
	- weighted arithmetic mean: 
  ~ Equation {#mean-default}
    x = \frac{\sum_{i=1}^n w_{i} x_{i}}{\sum_{i=1}^n w_i}
  ~
  - trimmed mean: chop extreme values; calculate mean afterwards

- Median
  - Middle value (if odd number of values) or average of the middle two values (otherwise)
- Mode
  - Value that occurs most often in data
  - unimodal, bimodal, trimodal are data sets with 1, 2, 3 modes
  - empirical formula for moderately asymmetrical curves:
  ~ Equation {#mod-asym-curves}
      mean - mode = 3 \cdot (mean - median)
  ~
- Standard Deviation
  - "Average Distance to Mean"
  ~ Equation {#standard-deviation}
    s = \sqrt{ \frac{\sum_{i=1}^n X_i - X_{mean}}{n - 1}}
  ~
- Variance
  ~ Equation {#variance}
    s = \frac{\sum_{i=1}^n (X_i - X_{mean})^2}{n - 1}
  ~
#Tutorial 1
##Different Data Types
*Categories: nominal, ordinal*

- nominal (kategorisierend):
	- Haarfarbe, Marke
- binary (entscheidend) - im Prinzip Subklasse von nominal:
	- Geschlecht
- ordinal (vergleichend):
	- Alter, Geschwindigkeit, Preis
- numeric
	- Quantity
	- Interval
		- measured on a scale of units
		- ordered
	- Ratio
		- inherent zero point

*Question the data you get... Do not believe everything which has a nice graphic*
##Different Visualization Techniques
- pie chart
- bar chart
- x-y-diagram
- ...

#Tutorial 2

##X²-Test
 - Measurement to test for relevance of nominal data relations
 - 

## correlation
~ Equation
  r_{A,B} = \frac{\sum_{i=1}^N (a_i - A_{exp})(b_i - B_{exp})}{N \cdot \theta_A \theta_B}
~
- $r_{A,B} > 0$: A and B are positively correlated (A increases as B). The higher $r_{A,B}$, the stronger the correlation.
- $r_{A,B} = 0$: independent
- $r_{A,B} < 0$: A and B are negatively correlated.
## covariance
~ Equation
  Cov(A,B) = \frac{\sum_{i=1}^n (X_i - X_{mean})(X_i - Y_{mean})}{n - 1}
  r_{A,B} = \frac{Cov(A,B)}{\theta_A \cdot \theta_B}
~
- $Cov_{A, B} > 0$: A and B tend to be greater than expected (positive covariance)
- $Cov_{A, B} = 0$: independent
- $Cov_{A, B} < 0$: negative covariance

## Feature Selection vs Feature Extraction (reduction)
### Feature Selection
- Univariate methods
  - mean / variance
### Feature Extraction
- Dimensionality Reduction
  - Principal Component Analysis
    1. Normalize (subtract mean)
    2. Calculate Covariance Matrix ($M_{i,j} = Cov(X_i, X_j)$)
    3. Calculate Eigenvectors, Eigenvalues (both normalized) for Cov Matrix
    4. Sort Eigenvectors by Eigenvalues -> determines relevance
    5. Create Feature Vector from most significant eigenvectors
    6. Calculate FeatureVector.T X NormalizedData.T
- Numerosity Reduction
  - Sampling
- Data Compression

#### Eigenvectors
- 

# Tutorial 3
## Evaluation of Classification Systems
### K-fold cross validation
  - divide data set in k partitions
  - use k-1 partitions for training, 1 partition for testing
  - rotate (use next k-1 part.s for training, next 1 part. for testing)
### confusion matrix

|----------------------|-----------------|-----------------|
|Actual $\rightarrow$  |Class 1          |Class 2         |
|`___________`         |                 |                 |
|Predicted $\downarrow$|                 |                 |
+:---------------------+:----------------+----------------:+
|Class 1               |A: True Positive |B: False Positive|
|----------------------|-----------------|-----------------|
|Class 2               |C: False Negative|D: True Negative |
|----------------------|-----------------|-----------------|

#### Evaluation Metrics:
- Accuracy:
  ~ Equation
    A = \frac{A+D}{A+B+C+D}
  ~
- True positive rate:
  ~ Equation
    TPr = \frac{A}{A+C} = 1 - false negative rate
  ~
- False positive rate:
  ~ Equation
    FPr = \frac{B}{B+D} = 1 - true negative rate
  ~
- Sensitivity:
  ~ Equation
    SE = TPr
  ~
- Specificity
  ~ Equation
    SP = 1 - FPr = TNr
  ~
- Recall:
  ~ Equation
    R = \frac{A}{A+C}
  ~
- Precision:
  ~ Equation
    P = \frac{A}{A+B}
  ~
- F-Score:
  ~ Equation
    F = \frac{2 \cdot P \cdot R}{P + R}
  ~
### Cost Matrix

|----------------------|----------------------|-----------------|-----------------|
|Cost Matrix           |Actual $\rightarrow$  |Class 1          |Class 2          |
+:---------------------+:--------------------:+:---------------:+:---------------:+
|Predicted $\downarrow$|C(1\|2)               |+                |-                |
|----------------------|----------------------|-----------------|-----------------|
|Class 1               |+                     |A: True Positive |B: False Positive|
|----------------------|----------------------|-----------------|-----------------|
|Class 2               |-                     |C: False Negative|D: True Negative |
|----------------------|----------------------|-----------------|-----------------|

## Apriori Algorithm
  1. Scan item set to create new candidate itemset of frequent items (k=1).
  2. Generate candidate set of combinations of k+1 frequent items
  3. Test the candidates for occurence in data set and prune if not present
  4. k = k+1
  5. Terminate, if no frequent or candidate set can be generated, else go to 2.

## Decision Trees
- Nodes: Tests on attributes
- Outgoing branches: corresponding to outcomes of tests at nodes (ergo: decisions)
- yes = and; no = or
- Leaves: classes

### Tree generation
1. construction
  - at start all training samples are at the root
  - recursively partition samples based on selected attributes
2. pruning
  - identify and remove branches that reflect noise or outliers

### Selecting Attributes
#### Entropy
- Measure of uncertainty
- Calculation: for a discrete random variable Y taking m distinct values {y1, ..., ym}
~ Equation
  H(Y) = -\sum_{i=1}^m p_i \cdot log(p_i) \text{ where } p_i = P(Y=y_i)
~

#### Information Gain
- **Information** [*Info(D)*] == Entropy, where D = Data Set
- Information still needed to classify D (after making decision based on Attribute A to split D into v partitions):
  ~ Equation
    Info_A(D) = \sum_{j=1}^{v} \frac{|D_j|}{|D|} \cdot Info(D_j)
  ~
- **Information gained** by branching on A:
  ~ Equation
    Gain(A) = Info(D) - Info_A(D)
  ~ 
## Neural Networks
- Perceptron
- MLP
  - Backpropagation Algorithm
    --> TODO
- Meaning of Learning Rate
- Validation
  - Three set: Training, Validation, Test
  - Cross-Validation: Swap around Training and Test sets
- Early stopping
- Weaknesses:
  - Long training time
  - require number of empirically determined parameters (network topology)
  - challenging to interpret symbolic meaning behind weights and "hidden" topology
  - Bad handling of missing data
- Strengths:
  - well suited for continuos valued inputs and outputs
  - High tolerance to noisy data
  - generalisation - classify untrained patterns
  - successful on a wide array of real world data
  - inherenctly parallel
  - possible to extract rules from trained neural networks
  - relationship to brain

## Cluster Analysis
- Cluster: Group of data objects
  - similar (or related) to one another within the same group
  - dissimilar (or unrelated) to the objects in other groups
- Cluster analysis
  - Determining clusters based on patterns in the data
  - **unsupervised learning**
- Typical applications:
  - As *standalone tool* to get insight into data distribution
  - As *preprocessing step* for other algorithms