Title         : Data Mining Tutorial Summary
Author        : Tobias Knöppler

[TITLE]

#Measurements
*n = sample Size; N = population size*

- Mean (sample vs population)
  ~ Equation {#mean-default}
    x = \frac{1}{n}\sum_{i=1}^n x_i
  ~
	- weighted arithmetic mean: 
  ~ Equation {#mean-default}
    x = \frac{\sum_{i=1}^n w_{i} x_{i}}{\sum_{i=1}^n w_i}
  ~
  - trimmed mean: chop extreme values; calculate mean afterwards

- Median
  - Middle value (if odd number of values) or average of the middle two values (otherwise)
- Mode
  - Value that occurs most often in data
  - unimodal, bimodal, trimodal are data sets with 1, 2, 3 modes
  - empirical formula for moderately asymmetrical curves:
  ~ Equation {#mod-asym-curves}
      mean - mode = 3 \cdot (mean - median)
  ~
- Standard Deviation
  - "Average Distance to Mean"
  ~ Equation {#standard-deviation}
    s = \sqrt{ \frac{\sum_{i=1}^n X_i - X_{mean}}{n - 1}}
  ~
- Variance
  ~ Equation {#variance}
    s = \frac{\sum_{i=1}^n (X_i - X_{mean})^2}{n - 1}
  ~
#Tutorial 1
##Different Data Types
*Categories: nominal, ordinal*

- nominal (kategorisierend):
	- Haarfarbe, Marke
- binary (entscheidend) - im Prinzip Subklasse von nominal:
	- Geschlecht
- ordinal (vergleichend):
	- Alter, Geschwindigkeit, Preis
- numeric
	- Quantity
	- Interval
		- measured on a scale of units
		- ordered
	- Ratio
		- inherent zero point

*Question the data you get... Do not believe everything which has a nice graphic*
##Different Visualization Techniques
- pie chart
- bar chart
- x-y-diagram
- ...

#Tutorial 2

##X²-Test
 - Measurement to test for relevance of nominal data relations
 - 

## correlation
~ Equation
  r_{A,B} = \frac{\sum_{i=1}^N (a_i - A_{exp})(b_i - B_{exp})}{N \cdot \theta_A \theta_B}
~
- $r_{A,B} > 0$: A and B are positively correlated (A increases as B). The higher $r_{A,B}$, the stronger the correlation.
- $r_{A,B} = 0$: independent
- $r_{A,B} < 0$: A and B are negatively correlated.
## covariance
~ Equation
  Cov(A,B) = \frac{\sum_{i=1}^n (X_i - X_{mean})(X_i - Y_{mean})}{n - 1}
  r_{A,B} = \frac{Cov(A,B)}{\theta_A \cdot \theta_B}
~
- $Cov_{A, B} > 0$: A and B tend to be greater than expected (positive covariance)
- $Cov_{A, B} = 0$: independent
- $Cov_{A, B} < 0$: negative covariance

## Feature Selection vs Feature Extraction (reduction)
### Feature Selection
- Univariate methods
  - mean / variance
### Feature Extraction
- Dimensionality Reduction
  - Principal Component Analysis
    1. Normalize (subtract mean)
    2. Calculate Covariance Matrix ($M_{i,j} = Cov(X_i, X_j)$)
    3. Calculate Eigenvectors, Eigenvalues (both normalized) for Cov Matrix
    4. Sort Eigenvectors by Eigenvalues -> determines relevance
    5. Create Feature Vector from most significant eigenvectors
    6. Calculate FeatureVector.T X NormalizedData.T
- Numerosity Reduction
  - Sampling
- Data Compression

#### Eigenvectors
- 

# Tutorial 3
## Evaluation of Classification Systems
### K-fold cross validation
  - divide data set in k partitions
  - use k-1 partitions for training, 1 partition for testing
  - rotate (use next k-1 part.s for training, next 1 part. for testing)
### confusion matrix

|----------------------|-----------------|-----------------|
|Actual $\rightarrow$  |Class 1          |Class 2         |
|`___________`         |                 |                 |
|Predicted $\downarrow$|                 |                 |
+:---------------------+:----------------+----------------:+
|Class 1               |A: True Positive |B: False Positive|
|----------------------|-----------------|-----------------|
|Class 2               |C: False Negative|D: True Negative |
|----------------------|-----------------|-----------------|

#### Evaluation Metrics:
- Accuracy:
  ~ Equation
    A = \frac{A+D}{A+B+C+D}
  ~
- True positive rate:
  ~ Equation
    TPr = \frac{A}{A+C} = 1 - false negative rate
  ~
- False positive rate:
  ~ Equation
    FPr = \frac{B}{B+D} = 1 - true negative rate
  ~
- Sensitivity:
  ~ Equation
    SE = TPr
  ~
- Specificity
  ~ Equation
    SP = 1 - FPr = TNr
  ~
- Recall:
  ~ Equation
    R = \frac{A}{A+C}
  ~
- Precision:
  ~ Equation
    P = \frac{A}{A+B}
  ~
- F-Score:
  ~ Equation
    F = \frac{2 \cdot P \cdot R}{P + R}
  ~
### Cost Matrix

|----------------------|----------------------|-----------------|-----------------|
|Cost Matrix           |Actual $\rightarrow$  |Class 1          |Class 2          |
+:---------------------+:--------------------:+:---------------:+:---------------:+
|Predicted $\downarrow$|C(1\|2)               |+                |-                |
|----------------------|----------------------|-----------------|-----------------|
|Class 1               |+                     |A: True Positive |B: False Positive|
|----------------------|----------------------|-----------------|-----------------|
|Class 2               |-                     |C: False Negative|D: True Negative |
|----------------------|----------------------|-----------------|-----------------|

## Apriori Algorithm
  1. Scan item set to create new candidate itemset of frequent items (k=1).
  2. Generate candidate set of combinations of k+1 frequent items
  3. Test the candidates for occurence in data set and prune if not present
  4. k = k+1
  5. Terminate, if no frequent or candidate set can be generated, else go to 2.

## Decision Trees
- Nodes: Tests on attributes
- Outgoing branches: corresponding to outcomes of tests at nodes (ergo: decisions)
- yes = and; no = or
- Leaves: classes

### Tree generation
1. construction
  - at start all training samples are at the root
  - recursively partition samples based on selected attributes
2. pruning
  - identify and remove branches that reflect noise or outliers

### Selecting Attributes
#### Entropy
- Measure of uncertainty
- Calculation: for a discrete random variable Y taking m distinct values {y1, ..., ym}
~ Equation
  H(Y) = -\sum_{i=1}^m p_i \cdot log(p_i) \text{ where } p_i = P(Y=y_i)
~

#### Information Gain
- **Information** [*Info(D)*] == Entropy, where D = Data Set
- Information still needed to classify D (after making decision based on Attribute A to split D into v partitions):
  ~ Equation
    Info_A(D) = \sum_{j=1}^{v} \frac{|D_j|}{|D|} \cdot Info(D_j)
  ~
- **Information gained** by branching on A:
  ~ Equation
    Gain(A) = Info(D) - Info_A(D)
  ~ 
## Neural Networks
- Perceptron
- MLP
  - Backpropagation Algorithm
    --> TODO
- Meaning of Learning Rate
- Validation
  - Three set: Training, Validation, Test
  - Cross-Validation: Swap around Training and Test sets
- Early stopping
- Weaknesses:
  - Long training time
  - require number of empirically determined parameters (network topology)
  - challenging to interpret symbolic meaning behind weights and "hidden" topology
  - Bad handling of missing data
- Strengths:
  - well suited for continuos valued inputs and outputs
  - High tolerance to noisy data
  - generalisation - classify untrained patterns
  - successful on a wide array of real world data
  - inherenctly parallel
  - possible to extract rules from trained neural networks
  - relationship to brain

## Cluster Analysis
- Cluster: Group of data objects
  - similar (or related) to one another within the same group
  - dissimilar (or unrelated) to the objects in other groups
- Cluster analysis
  - Determining clusters based on patterns in the data
  - **unsupervised learning**
- Typical applications:
  - As *standalone tool* to get insight into data distribution
  - As *preprocessing step* for other algorithms

### K-Means
- Algorithm:

  1. Determine number of clusters to find -> k
  2. Place k cluster centres to random positions
  3. assign data points to closest cluster center
  4. move cluster centers to mean of data point positions assigned to this cluster:
     $\left(\mu_j = \frac{1}{N_j} \sum_{i=1}^{N_j} x_i \text{ where } N_j = \text{number of points in cluster j}\right)$
  5. until the assignments don't change

### (Kohonen-)Self-Organizing-Feature-Map ((K)SOM) 
- Algorithm

     1. Grab input node $i$
     2. find best fitting node BMU, $u$ (by euclidean distance to weight vecor)
     3. Update node weight vectors $w_v$ to "pull them closer to input vector":
     $w_v(t+1) = w_v(t) + \Theta (u, v, s) \cdot \alpha (s) (i - W_v(t)); ~ \Theta = \text{neighbourhood function}$
     4. increase s and repeat from first step, while $s < \lambda$

- Neighbourhood function ($\Theta$):
  Determines either, *if* a node is a neighbour to the BMU
  or the level of "neighourness".
  Depending on this value the node will be updated.

## Genetic Algorithms
- Gene-Analogy:

|----------|-----------------------------------|
|Nature    |Algorithm                          |
+----------+-----------------------------------+
|Chromosome|String                             |
|----------|-----------------------------------|
|Gene      |Elements or Features in the string |
|----------|-----------------------------------|
|Locus     |Position in the String             |
|----------|-----------------------------------|
|Allele    |Position Value from an alphabet    |
|----------|-----------------------------------|
|Genotype  |String Structure                   |
|----------|-----------------------------------|
|Phenotype |Set of characteristics (features)  |
|----------|-----------------------------------|

- Algorithm Phases:
  1. Encoding schemata
  2. Fitness Evaluation
  3. Testing if stopping condition met. If yes -> Halt
  4. Else, Parent Selection
  5. Crossover Operators
  6. Mutation operators
  
  -> Goto 2.

- Encoding Schemata:
  - cover all possible solutions
  - allow only valid solutions (if possible)
  - choose appropriate representation:
    - Bit-string: decode integers or real numbers
      - Problems: e.g. mutation changes values significantly
        - workaround: Gray coding: two successive values differ by 1 bit
      - Better direct representation of numbers
      - Bit-string often fine
  - Two meanings of representation:
    - Mapping between phenotype and genotype space: de-/encoding
    - Data structure used in genotype space
  - Initialization: Mostly randomly
  - Choose alphabet
    -often binary
- Fitness Function
  - Requirement to adapt for (compare error function from neural networks)
  - Basis for string selection
  - Synonyms: **evaluation** or **objective function**
  - Always problem-specific:
    - *Example*
      - Context: Minimize x²
      - Phenotype: $x \in N$
      - Genotype: $z$: binary representation of $x$
      - Fitness Function: fitness $f(z)$ of genotype z is defined as 1 divided by square of its corresponding phenotype,
       e.g.: $z = 0010 \rightarrow \text{phenotype: } x = 2 \rightarrow f(z) = \frac{1}{x^2} = 0.25$
- Parent Selection
  - Select individuals tp create new population (offspring)
  - Mainly based on fitness
  - often probabilistic
    - sum over all probabilities is 1.0
  - parent selection supports process of evolving better solutions over time
- Crossover
  - Synonym: Recombination
  - Crossover applied with probability $p_c$, e.g. $p_c \in [0.5, 1.0]$ otherwise parents are copied
  - Implementations:
  
  **n-Point Crossover:**
  - Split parents at n points and recombine segments
  - Positional bias:
    - n-Point Crossover tends to keep together genes located close to each other
    - One-Point can never keep together genes from opposite ends
  - Uniform Crossover
    - Swap genes based on vector of random values
    - probability: e.g. 0.5 for each single value
    - reverse probability vector for second child
  - Beyond Crossover
    - Arithmetic recombination
- Variation: Mutation
  - Always stochastic: random and unbiased changes
  - Modes:
    - Bitwise
    - Random resetting 
- Survivor Selection
  - Similarly to parent selection
  - Role: reduce *n* parents and *o* offsprings to *n* individuals that constitute the next generation
    - Fitter individuals more likely to survive
    - Weak individuals may also survive
  - Selection often deterministic based on age and/or fitness
- Stopping condition
  - optimum value known?
  - **Problem:** GA's are stochastic and may never fulfill that condition
  - Other possible criteria:
    - time
    - Max number of iterations
    - diversity of population too small