Title         : Data Mining Tutorial Summary
Author        : Tobias Knöppler

[TITLE]

#Measurements
*n = sample Size; N = population size*

- Mean (sample vs population)
  ~ Equation {#mean-default}
    x = \frac{1}{n}\sum_{i=1}^n x_i
  ~
	- weighted arithmetic mean: 
  ~ Equation {#mean-default}
    x = \frac{\sum_{i=1}^n {w_{i} x_{i}{\sum_{i=1}^n w_i}
  ~
  - trimmed mean: chop extreme values; calculate mean afterwards

- Median
  - Middle value (if odd number of values) or average of the middle two values (otherwise)
- Mode
  - Value that occurs most often in data
  - unimodal, bimodal, trimodal are data sets with 1, 2, 3 modes
  - empirical formula for moderately asymmetrical curves:
  ~ Equation {#mod-asym-curves}
      mean - mode = 3 \cdot (mean - median)
  ~
- Standard Deviation
  - "Average Distance to Mean"
  ~ Equation {#standard-deviation}
    s = \sqrt{ \frac{\sum_{i=1}^n X_i - X_{mean}}{n - 1}}
  ~
- Variance
  ~ Equation {#variance}
    s = { \frac{\sum_{i=1}^n (X_i - X_{mean})^2}{n - 1}
  ~
#Tutorial 1
##Different Data Types
*Categories: nominal, ordinal*

- nominal (kategorisierend):
	- Haarfarbe, Marke
- binary (entscheidend) - im Prinzip Subklasse von nominal:
	- Geschlecht
- ordinal (vergleichend):
	- Alter, Geschwindigkeit, Preis
- numeric
	- Quantity
	- Interval
		- measured on a scale of units
		- ordered
	- Ratio
		- inherent zero point

*Question the data you get... Do not believe everything which has a nice graphic*
##Different Visualization Techniques
- pie chart
- bar chart
- x-y-diagram
- ...

#Tutorial 2

##X²-Test
 - Measurement to test for relevance of nominal data relations
 - 

## correlation
~ Equation
  r_{A,B} = \frac{\sum_{i=1}^N (a_i - A_{exp})(b_i - B_{exp})}{N \cdot \teta_A \teta_B}
~
- $r_{A,B} > 0$: A and B are positively correlated (A increases as B). The higher $r_{A,B}$, the stronger the correlation.
- $r_{A,B} = 0$: independent
- $r_{A,B} < 0$: A and B are negatively correlated.
## covariance
~ Equation
  Cov(A,B) = { \frac{\sum_{i=1}^n (X_i - X_{mean})(X_i - Y_{mean})}{n - 1}
  r_{A,B} = \frac{Cov(A,B)}{\teta_A \cdot \teta_B}
~
- $Cov_{A, B} > 0$: A and B tend to be greater than expected (positive covariance)
- $Cov_{A, B} = 0$: independent
- $Cov_{A, B} < 0$: negative covariance

## Feature Selection vs Feature Extraction (reduction)
### Feature Selection
- Univariate methods
  - mean / variance
### Feature Extraction
- Dimensionality Reduction
  - Principal Component Analysis
    1. Normalize (subtract mean)
    2. Calculate Covariance Matrix ($M_{i,j} = Cov(X_i, X_j)$)
    3. Calculate Eigenvectors, Eigenvalues (both normalized) for Cov Matrix
    4. Sort Eigenvectors by Eigenvalues -> determines relevance
    5. Create Feature Vector from most significant eigenvectors
    6. Calculate FeatureVector.T X NormalizedData.T
- Numerosity Reduction
  - Sampling
- Data Compression

#### Eigenvectors
- 

# Tutorial 3
## Evaluation of Classification Systems
### K-fold cross validation
  - divide data set in k partitions
  - use k-1 partitions for training, 1 partition for testing
  - rotate (use next k-1 part.s for training, next 1 part. for testing)
### confusion matrix
           Actual >|     Class 1     |     Class 2     |
        Predicted V|-----------------|-----------------|
           Class 1 |A: True Positive |B: False Positive|
           Class 2 |C: False Negative|D: True Negative |
                   |-----------------|-----------------|

#### Evaluation Metrics:
- Accuracy:
  ~ Equation
    A = \frac{A+D}{A+B+C+D}
  ~
- True positive rate:
  ~ Equation
    TPr = \frac{A}{A+C} = 1 - false negative rate
  ~
- False positive rate:
  ~ Equation
    FPr = \frac{B}{B+D} = 1 - true negative rate
  ~
- Sensitivity:
  ~ Equation
    SE = TPr
  ~
- Specificity
  ~ Equation
    SP = 1 - FPr = TNr
  ~
- Recall:
  ~ Equation
    R = \frac{A}{A+C}
  ~
- Precision:
  ~ Equation
    P = \frac{A}{A+B}
  ~
- F-Score:
  ~ Equation
    F = \frac{2 \cdot P \cdot R}{P + R}
  ~
### Cost Matrix
       Cost Matrix |Actual |     Class 1     |     Class 2     |
           --------|-------|-----------------|-----------------|
         Predicted |C(1|2) |        +        |        -        |
           Class 1 |   +   |A: True Positive |B: False Positive|
           Class 2 |   -   |C: False Negative|D: True Negative |
                   |-------|-----------------|-----------------|

### Apriori Algorithm
    1. Scan item set to create new candidate itemset of frequent items (k=1).
    2. Generate candidate set of combinations of k+1 frequent items
    3. Test the candidates for occurence in data set and prune if not present
    4. k = k+1
    5. Terminate, if no frequent or candidate set can be generated, else go to 2.